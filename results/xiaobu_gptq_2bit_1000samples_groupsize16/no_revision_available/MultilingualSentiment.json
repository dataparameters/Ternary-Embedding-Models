{
  "dataset_revision": "46958b007a63fdbf239b7672c25d0bea67b5ea1a",
  "evaluation_time": 59.45865273475647,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.18",
  "scores": {
    "test": [
      {
        "accuracy": 0.7706333333333334,
        "f1": 0.769902888859004,
        "f1_weighted": 0.769902888859004,
        "hf_subset": "default",
        "languages": [
          "cmn-Hans"
        ],
        "main_score": 0.7706333333333334,
        "scores_per_experiment": [
          {
            "accuracy": 0.7546666666666667,
            "f1": 0.7581622707153371,
            "f1_weighted": 0.7581622707153371
          },
          {
            "accuracy": 0.7406666666666667,
            "f1": 0.7326617986018397,
            "f1_weighted": 0.7326617986018397
          },
          {
            "accuracy": 0.776,
            "f1": 0.7737435156063904,
            "f1_weighted": 0.7737435156063904
          },
          {
            "accuracy": 0.773,
            "f1": 0.7737906659017496,
            "f1_weighted": 0.7737906659017496
          },
          {
            "accuracy": 0.7813333333333333,
            "f1": 0.7815847539799056,
            "f1_weighted": 0.7815847539799056
          },
          {
            "accuracy": 0.7536666666666667,
            "f1": 0.7544236982641092,
            "f1_weighted": 0.7544236982641089
          },
          {
            "accuracy": 0.774,
            "f1": 0.7728610600189443,
            "f1_weighted": 0.7728610600189444
          },
          {
            "accuracy": 0.7863333333333333,
            "f1": 0.7848877435369445,
            "f1_weighted": 0.7848877435369443
          },
          {
            "accuracy": 0.767,
            "f1": 0.7665949651602014,
            "f1_weighted": 0.7665949651602013
          },
          {
            "accuracy": 0.7996666666666666,
            "f1": 0.80031841680462,
            "f1_weighted": 0.8003184168046199
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.759,
        "f1": 0.7582770824159,
        "f1_weighted": 0.7582770824159001,
        "hf_subset": "default",
        "languages": [
          "cmn-Hans"
        ],
        "main_score": 0.759,
        "scores_per_experiment": [
          {
            "accuracy": 0.7526666666666667,
            "f1": 0.7568698636598591,
            "f1_weighted": 0.7568698636598591
          },
          {
            "accuracy": 0.7396666666666667,
            "f1": 0.7314852908593892,
            "f1_weighted": 0.7314852908593891
          },
          {
            "accuracy": 0.7653333333333333,
            "f1": 0.762702887612572,
            "f1_weighted": 0.762702887612572
          },
          {
            "accuracy": 0.756,
            "f1": 0.7571108022880423,
            "f1_weighted": 0.7571108022880423
          },
          {
            "accuracy": 0.754,
            "f1": 0.7540209024710637,
            "f1_weighted": 0.7540209024710636
          },
          {
            "accuracy": 0.7476666666666667,
            "f1": 0.7488616391335438,
            "f1_weighted": 0.7488616391335439
          },
          {
            "accuracy": 0.7573333333333333,
            "f1": 0.7557106629019598,
            "f1_weighted": 0.7557106629019598
          },
          {
            "accuracy": 0.7693333333333333,
            "f1": 0.7679655413567296,
            "f1_weighted": 0.7679655413567297
          },
          {
            "accuracy": 0.7706666666666667,
            "f1": 0.7701390948101067,
            "f1_weighted": 0.7701390948101068
          },
          {
            "accuracy": 0.7773333333333333,
            "f1": 0.7779041390657339,
            "f1_weighted": 0.777904139065734
          }
        ]
      }
    ]
  },
  "task_name": "MultilingualSentiment"
}