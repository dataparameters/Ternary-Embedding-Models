{
  "dataset_revision": "46958b007a63fdbf239b7672c25d0bea67b5ea1a",
  "evaluation_time": 59.61609435081482,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.18",
  "scores": {
    "test": [
      {
        "accuracy": 0.7589666666666666,
        "f1": 0.7580301367212243,
        "f1_weighted": 0.7580301367212243,
        "hf_subset": "default",
        "languages": [
          "cmn-Hans"
        ],
        "main_score": 0.7589666666666666,
        "scores_per_experiment": [
          {
            "accuracy": 0.7413333333333333,
            "f1": 0.7445530520896585,
            "f1_weighted": 0.7445530520896585
          },
          {
            "accuracy": 0.7193333333333334,
            "f1": 0.7089296034480753,
            "f1_weighted": 0.7089296034480753
          },
          {
            "accuracy": 0.764,
            "f1": 0.7629600297007744,
            "f1_weighted": 0.7629600297007743
          },
          {
            "accuracy": 0.7626666666666667,
            "f1": 0.764101262838993,
            "f1_weighted": 0.764101262838993
          },
          {
            "accuracy": 0.7486666666666667,
            "f1": 0.7500301889232013,
            "f1_weighted": 0.7500301889232013
          },
          {
            "accuracy": 0.7553333333333333,
            "f1": 0.7553446935482885,
            "f1_weighted": 0.7553446935482885
          },
          {
            "accuracy": 0.7723333333333333,
            "f1": 0.7708895713168212,
            "f1_weighted": 0.7708895713168212
          },
          {
            "accuracy": 0.774,
            "f1": 0.7723524891456695,
            "f1_weighted": 0.7723524891456695
          },
          {
            "accuracy": 0.7653333333333333,
            "f1": 0.7642550956525653,
            "f1_weighted": 0.7642550956525653
          },
          {
            "accuracy": 0.7866666666666666,
            "f1": 0.7868853805481967,
            "f1_weighted": 0.7868853805481966
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.7454333333333334,
        "f1": 0.7445560123999977,
        "f1_weighted": 0.7445560123999977,
        "hf_subset": "default",
        "languages": [
          "cmn-Hans"
        ],
        "main_score": 0.7454333333333334,
        "scores_per_experiment": [
          {
            "accuracy": 0.7373333333333333,
            "f1": 0.7409035180621468,
            "f1_weighted": 0.7409035180621467
          },
          {
            "accuracy": 0.7196666666666667,
            "f1": 0.7099945203322785,
            "f1_weighted": 0.7099945203322785
          },
          {
            "accuracy": 0.7386666666666667,
            "f1": 0.737542971626266,
            "f1_weighted": 0.7375429716262659
          },
          {
            "accuracy": 0.746,
            "f1": 0.7470280626249806,
            "f1_weighted": 0.7470280626249806
          },
          {
            "accuracy": 0.738,
            "f1": 0.7400271931334949,
            "f1_weighted": 0.7400271931334947
          },
          {
            "accuracy": 0.746,
            "f1": 0.7460294328156412,
            "f1_weighted": 0.746029432815641
          },
          {
            "accuracy": 0.742,
            "f1": 0.7390842181214566,
            "f1_weighted": 0.7390842181214566
          },
          {
            "accuracy": 0.763,
            "f1": 0.7623872091030481,
            "f1_weighted": 0.7623872091030481
          },
          {
            "accuracy": 0.7626666666666667,
            "f1": 0.7616495152092418,
            "f1_weighted": 0.7616495152092421
          },
          {
            "accuracy": 0.761,
            "f1": 0.760913482971422,
            "f1_weighted": 0.7609134829714221
          }
        ]
      }
    ]
  },
  "task_name": "MultilingualSentiment"
}