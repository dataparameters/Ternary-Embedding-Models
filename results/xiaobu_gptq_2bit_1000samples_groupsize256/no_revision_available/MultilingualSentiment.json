{
  "dataset_revision": "46958b007a63fdbf239b7672c25d0bea67b5ea1a",
  "evaluation_time": 59.456663608551025,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.18",
  "scores": {
    "test": [
      {
        "accuracy": 0.7250666666666665,
        "f1": 0.7223383710112979,
        "f1_weighted": 0.722338371011298,
        "hf_subset": "default",
        "languages": [
          "cmn-Hans"
        ],
        "main_score": 0.7250666666666665,
        "scores_per_experiment": [
          {
            "accuracy": 0.708,
            "f1": 0.7106395246976392,
            "f1_weighted": 0.7106395246976392
          },
          {
            "accuracy": 0.699,
            "f1": 0.6856478441478493,
            "f1_weighted": 0.6856478441478492
          },
          {
            "accuracy": 0.7333333333333333,
            "f1": 0.7313283737388754,
            "f1_weighted": 0.7313283737388755
          },
          {
            "accuracy": 0.7193333333333334,
            "f1": 0.7188269913874658,
            "f1_weighted": 0.7188269913874655
          },
          {
            "accuracy": 0.7353333333333333,
            "f1": 0.7349479074763924,
            "f1_weighted": 0.7349479074763925
          },
          {
            "accuracy": 0.7206666666666667,
            "f1": 0.7155135768403422,
            "f1_weighted": 0.7155135768403423
          },
          {
            "accuracy": 0.7196666666666667,
            "f1": 0.7151308832154141,
            "f1_weighted": 0.7151308832154141
          },
          {
            "accuracy": 0.7476666666666667,
            "f1": 0.7459657178218463,
            "f1_weighted": 0.7459657178218463
          },
          {
            "accuracy": 0.7266666666666667,
            "f1": 0.7266902502196619,
            "f1_weighted": 0.726690250219662
          },
          {
            "accuracy": 0.741,
            "f1": 0.738692640567494,
            "f1_weighted": 0.7386926405674938
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.7085666666666668,
        "f1": 0.7059124902583933,
        "f1_weighted": 0.7059124902583933,
        "hf_subset": "default",
        "languages": [
          "cmn-Hans"
        ],
        "main_score": 0.7085666666666668,
        "scores_per_experiment": [
          {
            "accuracy": 0.6973333333333334,
            "f1": 0.7013349352160629,
            "f1_weighted": 0.7013349352160629
          },
          {
            "accuracy": 0.6933333333333334,
            "f1": 0.678981179069244,
            "f1_weighted": 0.6789811790692439
          },
          {
            "accuracy": 0.7083333333333334,
            "f1": 0.7063309395038774,
            "f1_weighted": 0.7063309395038773
          },
          {
            "accuracy": 0.7006666666666667,
            "f1": 0.7008548180035993,
            "f1_weighted": 0.7008548180035993
          },
          {
            "accuracy": 0.7126666666666667,
            "f1": 0.7120689791357605,
            "f1_weighted": 0.7120689791357605
          },
          {
            "accuracy": 0.6976666666666667,
            "f1": 0.6931427915858747,
            "f1_weighted": 0.6931427915858748
          },
          {
            "accuracy": 0.701,
            "f1": 0.6963167879784407,
            "f1_weighted": 0.6963167879784405
          },
          {
            "accuracy": 0.7266666666666667,
            "f1": 0.7263915699855179,
            "f1_weighted": 0.726391569985518
          },
          {
            "accuracy": 0.7253333333333334,
            "f1": 0.7245092279540731,
            "f1_weighted": 0.7245092279540731
          },
          {
            "accuracy": 0.7226666666666667,
            "f1": 0.7191936741514827,
            "f1_weighted": 0.7191936741514828
          }
        ]
      }
    ]
  },
  "task_name": "MultilingualSentiment"
}