{
  "dataset_revision": "46958b007a63fdbf239b7672c25d0bea67b5ea1a",
  "evaluation_time": 28.60092306137085,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.17",
  "scores": {
    "test": [
      {
        "accuracy": 0.7051666666666667,
        "f1": 0.7044920842612431,
        "f1_weighted": 0.7044920842612429,
        "hf_subset": "default",
        "languages": [
          "cmn-Hans"
        ],
        "main_score": 0.7051666666666667,
        "scores_per_experiment": [
          {
            "accuracy": 0.7,
            "f1": 0.7039462795560357,
            "f1_weighted": 0.7039462795560356
          },
          {
            "accuracy": 0.6816666666666666,
            "f1": 0.6774549711336366,
            "f1_weighted": 0.6774549711336366
          },
          {
            "accuracy": 0.6996666666666667,
            "f1": 0.696667518655293,
            "f1_weighted": 0.696667518655293
          },
          {
            "accuracy": 0.7126666666666667,
            "f1": 0.7142879076184464,
            "f1_weighted": 0.7142879076184463
          },
          {
            "accuracy": 0.713,
            "f1": 0.7138314143669063,
            "f1_weighted": 0.7138314143669063
          },
          {
            "accuracy": 0.704,
            "f1": 0.7042173970127411,
            "f1_weighted": 0.7042173970127411
          },
          {
            "accuracy": 0.6856666666666666,
            "f1": 0.6843072644870665,
            "f1_weighted": 0.6843072644870665
          },
          {
            "accuracy": 0.7326666666666667,
            "f1": 0.7295801436100192,
            "f1_weighted": 0.7295801436100191
          },
          {
            "accuracy": 0.6986666666666667,
            "f1": 0.6955041598700294,
            "f1_weighted": 0.6955041598700293
          },
          {
            "accuracy": 0.7236666666666667,
            "f1": 0.7251237863022554,
            "f1_weighted": 0.7251237863022555
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.6856333333333333,
        "f1": 0.6851347475053541,
        "f1_weighted": 0.685134747505354,
        "hf_subset": "default",
        "languages": [
          "cmn-Hans"
        ],
        "main_score": 0.6856333333333333,
        "scores_per_experiment": [
          {
            "accuracy": 0.6676666666666666,
            "f1": 0.6734905708853605,
            "f1_weighted": 0.6734905708853605
          },
          {
            "accuracy": 0.6566666666666666,
            "f1": 0.6519307427369192,
            "f1_weighted": 0.6519307427369192
          },
          {
            "accuracy": 0.6866666666666666,
            "f1": 0.6836321201715858,
            "f1_weighted": 0.6836321201715859
          },
          {
            "accuracy": 0.68,
            "f1": 0.681598287617124,
            "f1_weighted": 0.681598287617124
          },
          {
            "accuracy": 0.6936666666666667,
            "f1": 0.6941091328700247,
            "f1_weighted": 0.6941091328700246
          },
          {
            "accuracy": 0.691,
            "f1": 0.6916818818601183,
            "f1_weighted": 0.691681881860118
          },
          {
            "accuracy": 0.6646666666666666,
            "f1": 0.6633386947402652,
            "f1_weighted": 0.6633386947402652
          },
          {
            "accuracy": 0.7096666666666667,
            "f1": 0.7080230086088308,
            "f1_weighted": 0.7080230086088307
          },
          {
            "accuracy": 0.6963333333333334,
            "f1": 0.6920468275957078,
            "f1_weighted": 0.6920468275957078
          },
          {
            "accuracy": 0.71,
            "f1": 0.7114962079676044,
            "f1_weighted": 0.7114962079676043
          }
        ]
      }
    ]
  },
  "task_name": "MultilingualSentiment"
}