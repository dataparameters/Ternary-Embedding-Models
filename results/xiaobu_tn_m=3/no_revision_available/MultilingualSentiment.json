{
  "dataset_revision": "46958b007a63fdbf239b7672c25d0bea67b5ea1a",
  "evaluation_time": 63.65314531326294,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.18",
  "scores": {
    "test": [
      {
        "accuracy": 0.7329333333333332,
        "f1": 0.7316504419085282,
        "f1_weighted": 0.7316504419085282,
        "hf_subset": "default",
        "languages": [
          "cmn-Hans"
        ],
        "main_score": 0.7329333333333332,
        "scores_per_experiment": [
          {
            "accuracy": 0.711,
            "f1": 0.7156021764980602,
            "f1_weighted": 0.7156021764980602
          },
          {
            "accuracy": 0.681,
            "f1": 0.6690009112570298,
            "f1_weighted": 0.6690009112570298
          },
          {
            "accuracy": 0.7233333333333334,
            "f1": 0.7206920655721641,
            "f1_weighted": 0.720692065572164
          },
          {
            "accuracy": 0.7523333333333333,
            "f1": 0.753835452926205,
            "f1_weighted": 0.7538354529262051
          },
          {
            "accuracy": 0.7323333333333333,
            "f1": 0.7346683539589041,
            "f1_weighted": 0.7346683539589043
          },
          {
            "accuracy": 0.732,
            "f1": 0.7328740924367798,
            "f1_weighted": 0.7328740924367797
          },
          {
            "accuracy": 0.728,
            "f1": 0.7253247704540545,
            "f1_weighted": 0.7253247704540545
          },
          {
            "accuracy": 0.7623333333333333,
            "f1": 0.7581582789839979,
            "f1_weighted": 0.758158278983998
          },
          {
            "accuracy": 0.747,
            "f1": 0.7448665537716782,
            "f1_weighted": 0.7448665537716781
          },
          {
            "accuracy": 0.76,
            "f1": 0.7614817632264078,
            "f1_weighted": 0.7614817632264078
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.7105666666666667,
        "f1": 0.7099496730359517,
        "f1_weighted": 0.7099496730359516,
        "hf_subset": "default",
        "languages": [
          "cmn-Hans"
        ],
        "main_score": 0.7105666666666667,
        "scores_per_experiment": [
          {
            "accuracy": 0.6903333333333334,
            "f1": 0.6960073681713547,
            "f1_weighted": 0.6960073681713546
          },
          {
            "accuracy": 0.6686666666666666,
            "f1": 0.6590626151017966,
            "f1_weighted": 0.6590626151017966
          },
          {
            "accuracy": 0.7046666666666667,
            "f1": 0.7033899940073804,
            "f1_weighted": 0.7033899940073803
          },
          {
            "accuracy": 0.7196666666666667,
            "f1": 0.7215341923108441,
            "f1_weighted": 0.721534192310844
          },
          {
            "accuracy": 0.7163333333333334,
            "f1": 0.7185377795787802,
            "f1_weighted": 0.7185377795787802
          },
          {
            "accuracy": 0.7076666666666667,
            "f1": 0.7099387393329355,
            "f1_weighted": 0.7099387393329356
          },
          {
            "accuracy": 0.703,
            "f1": 0.6995905686163492,
            "f1_weighted": 0.699590568616349
          },
          {
            "accuracy": 0.7366666666666667,
            "f1": 0.7330850663548661,
            "f1_weighted": 0.7330850663548661
          },
          {
            "accuracy": 0.7296666666666667,
            "f1": 0.7273676271890265,
            "f1_weighted": 0.7273676271890263
          },
          {
            "accuracy": 0.729,
            "f1": 0.7309827796961841,
            "f1_weighted": 0.7309827796961842
          }
        ]
      }
    ]
  },
  "task_name": "MultilingualSentiment"
}