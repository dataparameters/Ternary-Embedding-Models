{
  "dataset_revision": "46958b007a63fdbf239b7672c25d0bea67b5ea1a",
  "evaluation_time": 33.96479940414429,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.18",
  "scores": {
    "test": [
      {
        "accuracy": 0.6605666666666667,
        "f1": 0.660183155376654,
        "f1_weighted": 0.660183155376654,
        "hf_subset": "default",
        "languages": [
          "cmn-Hans"
        ],
        "main_score": 0.6605666666666667,
        "scores_per_experiment": [
          {
            "accuracy": 0.65,
            "f1": 0.6525266967511479,
            "f1_weighted": 0.6525266967511479
          },
          {
            "accuracy": 0.6236666666666667,
            "f1": 0.623510894661151,
            "f1_weighted": 0.623510894661151
          },
          {
            "accuracy": 0.6446666666666667,
            "f1": 0.63949987354805,
            "f1_weighted": 0.6394998735480499
          },
          {
            "accuracy": 0.6843333333333333,
            "f1": 0.6865748267866408,
            "f1_weighted": 0.686574826786641
          },
          {
            "accuracy": 0.6716666666666666,
            "f1": 0.6737283232198167,
            "f1_weighted": 0.6737283232198165
          },
          {
            "accuracy": 0.6526666666666666,
            "f1": 0.6531538567732673,
            "f1_weighted": 0.6531538567732674
          },
          {
            "accuracy": 0.649,
            "f1": 0.6487336263367071,
            "f1_weighted": 0.6487336263367071
          },
          {
            "accuracy": 0.6966666666666667,
            "f1": 0.6922235291896083,
            "f1_weighted": 0.6922235291896083
          },
          {
            "accuracy": 0.6493333333333333,
            "f1": 0.6458356645647685,
            "f1_weighted": 0.6458356645647685
          },
          {
            "accuracy": 0.6836666666666666,
            "f1": 0.6860442619353817,
            "f1_weighted": 0.6860442619353817
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.6486666666666665,
        "f1": 0.6482848044646444,
        "f1_weighted": 0.6482848044646443,
        "hf_subset": "default",
        "languages": [
          "cmn-Hans"
        ],
        "main_score": 0.6486666666666665,
        "scores_per_experiment": [
          {
            "accuracy": 0.6356666666666667,
            "f1": 0.6373915781734868,
            "f1_weighted": 0.6373915781734868
          },
          {
            "accuracy": 0.6193333333333333,
            "f1": 0.618230718861729,
            "f1_weighted": 0.6182307188617291
          },
          {
            "accuracy": 0.625,
            "f1": 0.6213212242744647,
            "f1_weighted": 0.6213212242744646
          },
          {
            "accuracy": 0.661,
            "f1": 0.6625507311959877,
            "f1_weighted": 0.6625507311959877
          },
          {
            "accuracy": 0.657,
            "f1": 0.6589565212125329,
            "f1_weighted": 0.6589565212125328
          },
          {
            "accuracy": 0.6533333333333333,
            "f1": 0.6550647960678735,
            "f1_weighted": 0.6550647960678735
          },
          {
            "accuracy": 0.6323333333333333,
            "f1": 0.6319181189333615,
            "f1_weighted": 0.6319181189333614
          },
          {
            "accuracy": 0.6786666666666666,
            "f1": 0.6755783835234017,
            "f1_weighted": 0.6755783835234016
          },
          {
            "accuracy": 0.6566666666666666,
            "f1": 0.6520218309072586,
            "f1_weighted": 0.6520218309072586
          },
          {
            "accuracy": 0.6676666666666666,
            "f1": 0.6698141414963473,
            "f1_weighted": 0.6698141414963473
          }
        ]
      }
    ]
  },
  "task_name": "MultilingualSentiment"
}