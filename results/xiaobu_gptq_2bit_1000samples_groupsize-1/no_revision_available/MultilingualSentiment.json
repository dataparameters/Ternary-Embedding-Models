{
  "dataset_revision": "46958b007a63fdbf239b7672c25d0bea67b5ea1a",
  "evaluation_time": 59.79890441894531,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.18",
  "scores": {
    "test": [
      {
        "accuracy": 0.7041,
        "f1": 0.7028392318510215,
        "f1_weighted": 0.7028392318510215,
        "hf_subset": "default",
        "languages": [
          "cmn-Hans"
        ],
        "main_score": 0.7041,
        "scores_per_experiment": [
          {
            "accuracy": 0.6833333333333333,
            "f1": 0.6837536682736479,
            "f1_weighted": 0.6837536682736478
          },
          {
            "accuracy": 0.6983333333333334,
            "f1": 0.6892732948735757,
            "f1_weighted": 0.6892732948735757
          },
          {
            "accuracy": 0.698,
            "f1": 0.6926814634489323,
            "f1_weighted": 0.6926814634489322
          },
          {
            "accuracy": 0.719,
            "f1": 0.7212412513861031,
            "f1_weighted": 0.7212412513861032
          },
          {
            "accuracy": 0.719,
            "f1": 0.7193236889490121,
            "f1_weighted": 0.719323688949012
          },
          {
            "accuracy": 0.697,
            "f1": 0.6968794638820018,
            "f1_weighted": 0.696879463882002
          },
          {
            "accuracy": 0.69,
            "f1": 0.6902430956517694,
            "f1_weighted": 0.6902430956517697
          },
          {
            "accuracy": 0.7153333333333334,
            "f1": 0.7152320982897492,
            "f1_weighted": 0.7152320982897493
          },
          {
            "accuracy": 0.693,
            "f1": 0.6911519166418266,
            "f1_weighted": 0.6911519166418266
          },
          {
            "accuracy": 0.728,
            "f1": 0.7286123771135965,
            "f1_weighted": 0.7286123771135966
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.6864333333333332,
        "f1": 0.6860914802426613,
        "f1_weighted": 0.6860914802426613,
        "hf_subset": "default",
        "languages": [
          "cmn-Hans"
        ],
        "main_score": 0.6864333333333332,
        "scores_per_experiment": [
          {
            "accuracy": 0.6623333333333333,
            "f1": 0.6642384496763459,
            "f1_weighted": 0.6642384496763458
          },
          {
            "accuracy": 0.685,
            "f1": 0.6784807778457672,
            "f1_weighted": 0.6784807778457672
          },
          {
            "accuracy": 0.685,
            "f1": 0.6812410593439168,
            "f1_weighted": 0.6812410593439168
          },
          {
            "accuracy": 0.6956666666666667,
            "f1": 0.698094132694867,
            "f1_weighted": 0.698094132694867
          },
          {
            "accuracy": 0.7103333333333334,
            "f1": 0.7102807638129655,
            "f1_weighted": 0.7102807638129656
          },
          {
            "accuracy": 0.6746666666666666,
            "f1": 0.6758088503152578,
            "f1_weighted": 0.6758088503152578
          },
          {
            "accuracy": 0.6656666666666666,
            "f1": 0.6664547008271255,
            "f1_weighted": 0.6664547008271254
          },
          {
            "accuracy": 0.6913333333333334,
            "f1": 0.6929183875982335,
            "f1_weighted": 0.6929183875982335
          },
          {
            "accuracy": 0.6963333333333334,
            "f1": 0.6944570715048451,
            "f1_weighted": 0.6944570715048451
          },
          {
            "accuracy": 0.698,
            "f1": 0.6989406088072888,
            "f1_weighted": 0.6989406088072887
          }
        ]
      }
    ]
  },
  "task_name": "MultilingualSentiment"
}